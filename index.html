---
layout: page
description: "My Homepage"
motto: "Research for a better world."
---
<hr>
<div style="line-height:1.5; text-align:justify; text-justify:inter-ideograph; font-size:16px; color:black;">
    <p>
        I'm Yanwei Li (<span style="font-family: KaiTi_GB2312;">李彦玮</span>), currently working as a Research Scientist on Foundation Model for Vision & Language at ByteDance Seed, San Jose, USA.
        Before that, I obtained Ph.D degree in The Chinese University of Hong Kong (CUHK), supervised by Prof. 
        <a href="http://jiaya.me/" style="color:rgb(51, 156, 255)">Jiaya Jia</a>.
    </p>

    <p>
        Previously, I spent wonderful time in NVIDIA, MEGVII, Horizon Robotics, and IBM Research. During these periods, I was fortunate to collaborate with several top researchers like Prof. 
        <a href="http://tensorlab.cms.caltech.edu/users/anima/" style="color:rgb(51, 156, 255)">Anima Anandkumar</a> (CalTech), and Prof. 
        <a href="https://www.cs.utoronto.ca/~fidler/" style="color:rgb(51, 156, 255)">Sanja Fidler</a> (UofT), and Dr. 
        <a href="https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en" style="color:rgb(51, 156, 255)">Jian Sun</a>.
    </p>

    <p>
        My research interest mainly focus on Multi-modality Foundation Model and Generative AI. My recent work includes 
        <a href="https://arxiv.org/abs/2505.07062" style="color:rgb(51, 156, 255)">Seed 1.5-VL</a>, 
        <a href="https://arxiv.org/pdf/2408.03326?" style="color:rgb(51, 156, 255)">LLaVA-OneVision</a>, 
        <a href="https://arxiv.org/pdf/2403.18814.pdf" style="color:rgb(51, 156, 255)">Mini-Gemini</a>, 
        <a href="https://arxiv.org/pdf/2311.17043.pdf" style="color:rgb(51, 156, 255)">LLaMA-VID</a>, 
        <a href="https://arxiv.org/pdf/2308.00692" style="color:rgb(51, 156, 255)">LISA</a>, and 
        <a href="https://arxiv.org/abs/2405.21075" style="color:rgb(51, 156, 255)">Video-MME</a>.
        More experiences about me please refer to 
        <a href="/publication/" style="color:rgb(51, 156, 255)"><b>Publication</b></a> and my 
        <a href="https://scholar.google.com/citations?user=I-UCPPcAAAAJ&hl=zh-CN&oi=ao" style="color:rgb(51, 156, 255)"><b>Google Scholar</b></a>.
    </p>
</div>

<hr>
<h2> Recent Update</h2>
<table frame=void rules=none>
    <b><span style="color:red"> 2025/07:</span></b> We release the paper, code, and data of <a href="https://arxiv.org/pdf/2506.24102" style="color:rgb(51, 156, 255)">DenseWorld-1M</a> for VLM training!<br>
    <b><span style="color:red"> 2025/06:</span></b> 2 papers on VLMs accepted to <b>ICCV 2025</b>. Congratulations to the co-authors!<br>
    <b><span style="color:red"> 2025/05:</span></b> 2 popular VLM benchmarks (VideoMME & MME-CoT) accepted to <b>CVPR 2025</b> and <b>ICML 2025</b>.<br>
    <b><span style="color:red"> 2025/05:</span></b> We release the paper, code, and demos of <a href="https://arxiv.org/abs/2505.07062" style="color:rgb(51, 156, 255)">Seed 1.5-VL</a>, a worldwide top VLM for image/multi-image/video!<br>
    <b><span style="color:red"> 2024/08:</span></b> We release the paper, code, and models of <a href="https://arxiv.org/pdf/2408.03326?" style="color:rgb(51, 156, 255)">LLaVA-OneVision</a>, a top VLM for image/multi-image/video!<br>
    <b><span style="color:red"> 2024/03:</span></b> We release the paper, code, and models of <a href="https://arxiv.org/pdf/2403.18814.pdf" style="color:rgb(51, 156, 255)">Mini-Gemini</a>, a top VLM for image!<br>
</table>
<hr>

<h2> Preprint</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Seed1-5-VL.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Seed1.5-VL Technical Report</span></h5>
            <b>Core Contributor</b> in Seed1.5-VL Team<br>
            <i>arXiv Preprint, 2025</i>
            <br>[<a href="https://arxiv.org/pdf/2505.07062" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/ByteDance-Seed/Seed1.5-VL" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://seed.bytedance.com/en/tech/seed1_5_vl" style="color:rgb(51, 156, 255)">homepage</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DenseWorld1M.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World</span></h5>
            Xiangtai Li*, Tao Zhang*, <b>Yanwei Li</b>*, Haobo Yuan, Shihao Chen, Yikang Zhou, et.al.<br>
            <i>arXiv Preprint, 2025</i>
            <br>[<a href="https://arxiv.org/pdf/2506.24102" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/lxtGH/DenseWorld-1M" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://mini-gemini.github.io/" style="color:rgb(51, 156, 255)">project</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Mini-Gemini.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</span></h5>
            <b>Yanwei Li</b>*, Yuechen Zhang*, Chengyao Wang*, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia<br>
            <i>arXiv Preprint, 2024</i>
            <br>[<a href="https://arxiv.org/pdf/2403.18814.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/MiniGemini" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://mini-gemini.github.io/" style="color:rgb(51, 156, 255)">project</a>]
    </td></tr>

</table>
<hr>

<h2> Journal Paper</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/LOV.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">LLaVA-OneVision: Easy Visual Task Transfer</span></h5>
            Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, <b>Yanwei Li</b>, Ziwei Liu, Chunyuan Li<br>
            <i>Transactions on Machine Learning Research <b>(TMLR)</b>, 2025</i>
            <br>[<a href="https://arxiv.org/pdf/2408.03326?" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/" style="color:rgb(51, 156, 255)">project</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/PanopticFCN.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision</span></h5>
            <b>Yanwei Li</b>, Hengshuang Zhao, Xiaojuan Qi, Yukang Chen, Lu Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2022</i>
            <br>[<a href="https://arxiv.org/pdf/2108.07682.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/yanwei-li/PanopticFCN" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/SA_AutoAug.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Scale-aware Automatic Augmentations for Object Detection with Dynamic Training</span></h5>
            Yukang Chen, Peizhen Zhang, Tao Kong, <b>Yanwei Li</b>, Xiangyu Zhang, Lu Qi, Jian Sun, Jiaya Jia<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2022</i>
            <br>[<a href="https://ieeexplore.ieee.org/abstract/document/9756374/" style="color:rgb(51, 156, 255)">paper</a>]
    </td></tr>
</table>
<hr>

<h2> Conference Paper</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/LLaMA-VID.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</span></h5>
            <b>Yanwei Li</b>*, Chengyao Wang*, Jiaya Jia<br>
            <i>European Conference on Computer Vision <b>(ECCV)</b>, 2024</i>
            <br>[<a href="https://arxiv.org/pdf/2311.17043.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/LLaMA-VID" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://llama-vid.github.io/" style="color:rgb(51, 156, 255)">project</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/LISA.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">LISA: Reasoning Segmentation via Large Language Model</span></h5>
            Xin Lai, Zhuotao Tian, Yukang Chen, <b>Yanwei Li</b>, Yuhui Yuan, Shu Liu, Jiaya Jia<br> 
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2024 <b><span style="color:red">(Oral)</span></b></i>
            <br>[<a href="https://arxiv.org/pdf/2308.00692.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/LISA" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/GPT4Tools.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</span></h5>
            Rui Yang*, Lin Song*, <b>Yanwei Li</b>, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan<br>
            <i>Advances in Neural Information Processing Systems <b>(NeurIPS)</b>, 2023</i>  
            <br>[<a href="https://arxiv.org/pdf/2305.18752.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/AILab-CVC/GPT4Tools" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DQTrack.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">End-to-end 3D Tracking with Decoupled Queries</span></h5>
            <b>Yanwei Li</b>, Zhiding Yu, Jonah Philion, Animashree Anandkumar, Sanja Fidler, Jiaya Jia, Jose Alvarez<br>
            <i>International Conference on Computer Vision <b>(ICCV)</b>, 2023</i>  
            <br>[<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_End-to-end_3D_Tracking_with_Decoupled_Queries_ICCV_2023_paper.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/NVlabs/DQTrack" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/UVTR.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Unifying Voxel-based Representation with Transformer for 3D Object Detection</span></h5>
            <b>Yanwei Li</b>, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, Jiaya Jia<br>
            <i>Advances in Neural Information Processing Systems <b>(NeurIPS)</b>, 2022</i> 
            <br>[<a href="https://arxiv.org/pdf/2206.00630.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/UVTR" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/VFF.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Voxel Field Fusion for 3D Object Detection</span></h5>
            <b>Yanwei Li</b>, Xiaojuan Qi, Yukang Chen, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2022</i>
            <br>[<a href="https://arxiv.org/pdf/2205.15938.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/VFF" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/FSparseConv.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Focal Sparse Convolutional Networks for 3D Object Detection</span></h5>
            Yukang Chen, <b>Yanwei Li</b>, Xiangyu Zhang, Jian Sun, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2022 <b><span style="color:red">(Oral)</span></b></i>
            <br>[<a href="https://arxiv.org/pdf/2204.12463.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/FocalsConv" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/PanopticFCN.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Fully Convolutional Networks for Panoptic Segmentation</span></h5>
            <b>Yanwei Li</b>, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2021 <b><span style="color:red">(Oral)</span></b></i>
            <br>[<a href="https://arxiv.org/pdf/2012.00720.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/yanwei-li/PanopticFCN" style="color:rgb(51, 156, 255)">code</a>] [<a href="/talk/cvpr2021-talk.pdf" style="color:rgb(51, 156, 255)">slides</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/SA_AutoAug.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Scale-aware Automatic Augmentation for Object Detection</span></h5>
            Yukang Chen*, <b>Yanwei Li*</b>, Tao Kong, Lu Qi, Ruihang Chu, Lei Li, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2021</i>
            <br>[<a href="https://arxiv.org/pdf/2103.17220.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/Jia-Research-Lab/SA-AutoAug" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DynamicHead.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Fine-Grained Dynamic Head for Object Detection</span></h5>
            Lin Song, <b>Yanwei Li</b>, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun, Nanning Zheng<br>
            <i>Advances in Neural Information Processing Systems <b>(NeurIPS)</b>, 2020</i> 
            <br>[<a href="https://papers.nips.cc/paper/2020/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/StevenGrove/DynamicHead" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Tree_filterV2.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Rethinking Learnable Tree Filter for Generic Feature Transform</span></h5>
            Lin Song, <b>Yanwei Li</b>, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng<br>
            <i>Advances in Neural Information Processing Systems <b>(NeurIPS)</b>, 2020</i> 
            <br>[<a href="https://papers.nips.cc/paper/2020/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/StevenGrove/LearnableTreeFilterV2" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
    <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DynamicRouting.png" class="papericon"></td>
    <td class="pub_td2" style="width: 600px;">
        <h5><span style="color:black">Learning Dynamic Routing for Semantic Segmentation</span></h5>
        <b>Yanwei Li</b>, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, Jian Sun<br>
        <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2020 <b><span style="color:red">(Oral)</span></b></i>
        <br>[<a href="https://arxiv.org/abs/2003.10401" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/yanwei-li/DynamicRouting" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://www.youtube.com/watch?v=WHSBsoG4Agk" style="color:rgb(51, 156, 255)">video</a>] [<a href="/talk/cvpr2020-talk.pdf" style="color:rgb(51, 156, 255)">slides</a>]
    </td></tr>

<!-- <hr>
<h2> Education</h2>
<p style = "margin-top:15px">
    <font size=4>
        <b><span style="color:black">The Chinese University of Hong Kong</span></b><br/>
    </font>
    <b>Fields: Computer Vision</b><br/>
    <i>Ph.D, Computer Science and Engineering, 2020 - 2024.</i>
</p>

<p style = "margin-top:15px">
<font size=4>
    <b><span style="color:black">Institute of Automation, Chinese Academy of Sciences</span></b><br/>
</font>
<b>Fields: Computer Vision</b><br/>
<i>M.Phil, Pattern Recognition and Intelligent System, 2017 - 2020.</i>
</p>

<p style = "margin-top:5px">
<font size=4>
    <b><span style="color:black">Central South University</span></b><br/>
</font>
<i>B.E., Automation & Mechanical Engineering, 2013 - 2017.</i>
</p> -->
<hr>

<h2> Experience</h2>
<p style = "margin-top:15px">
<font size=4>
    <b><span style="color:black">ByteDance Seed</span></b><br/>
</font>
<b>Fields: Multimodal Foundation Model.</b><br/>
<i>Research Scientist, 2024.09 - Now</i><br/>
</p>

<p style = "margin-top:15px">
<font size=4>
    <b><span style="color:black">NVIDIA Research</span></b><br/>
</font>
<b>Fields: Multimodal Perception.</b><br/>
<i>Research Intern, 2022.06 - 2023.04</i><br/>
</p>

<p style = "margin-top:15px">
<font size=4>
    <b><span style="color:black">MEGVII</span></b><br/>
</font>
<b>Fields: 2D & 3D Perception.</b><br/>
<i>Research Intern, 2019.01 - 2022.05</i><br/>
</p>

<!-- <p style = "margin-top:5px">
<font size=4>
    <b><span style="color:black">Horizon Robotics</span></b><br/>
</font>
<b>Fields: 2D Perception.</b><br/>
<i>Research Intern, 2018.04 - 2018.12</i><br/>
</p>

<p style = "margin-top:5px">
<font size=4>
    <b><span style="color:black">IBM Research</span></b><br/>
</font>
<b>Fields:2D Perception.</b><br/>
<i>Research Intern, 2017.08 - 2018.01</i><br/>
</p> -->
<hr>

<h2> Service</h2>
<p style = "margin-top:15px">
<font size=4>
<span style="color:black">
    <b>Area Chair:</b><br/>
    Neural Information Processing Systems (NeurIPS), 2025.<br/>
</p>
    <b>Conference Reviewer:</b><br/>
    International Conference on Learning Representations (ICLR).<br/>
    International Conference on Machine Learning (ICML).<br/>
    Neural Information Processing Systems (NeurIPS).<br/>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).<br/>
    IEEE International Conference on Computer Vision (ICCV).<br/>
    European Conference on Computer Vision (ECCV).<br/>
    AAAI Conference on Artificial Intelligence (AAAI).<br/>
</p>
    <b>Journal Reviewer:</b><br/> 
    IEEE Transactions on Pattern Analysis and Machine Intelligence.<br/>
    International Journal of Computer Vision.<br/>
    IEEE Transactions on Image Processing.<br/>
    Pattern Recognition.<br/>
</span>
</font>
</p>
<hr>

<h2> Activity</h2>
<p style = "margin-top:15px">
<font size=4>
<span style="color:black">
    <b>Academic Talk:</b><br/>
    "LLaMA-VID:An Image is Worth 2 Tokens in Large Language Models", MIT/Huawei/Tencent, 2023. [<a href="talk/LLaMA-VID.pdf" style="color:rgb(51, 156, 255)">slides</a>]<br/>
    "Representation for Multi-modality 3D Detection with Transformer", ZhiDongXi, 2022. [<a href="talk/unfied_3d_det.pdf" style="color:rgb(51, 156, 255)">slides</a>]<br/> 
    "Towards Fully Convolutional Panoptic Segmentation", ByteDance AI & BAAI, 2021. [<a href="talk/PanopticFCN-talk.pdf" style="color:rgb(51, 156, 255)">slides</a>]<br/> 
    "Dynamic Network and Semantic Segmentation", Paper Weekly, 2020. [<a href="talk/DynamicNet-and-SemanticSeg-talk.pdf" style="color:rgb(51, 156, 255)">slides</a>]<br/> 
    "FPN-based Network for Panoptic Segmentation", ECCV COCO Workshop, 2018. [<a href="talk/eccv2018-panoptic-talk-Pro.pdf" style="color:rgb(51, 156, 255)">slides</a>]<br/> 
</p>
    <b>Teaching Assistant:</b><br/>
    CSCI1580: Visual Programming, Fall, 2022.<br/>
    ENGG5104: Image Processing and Computer Vision, Spring, 2022.<br/>
    CSCI1580: Visual Programming, Fall, 2021.<br/>
    CSCI2100B: Data Structures, Spring, 2021.<br/>
</span>
</font>
</p>
<hr>

<h2> Award</h2>
<p style = "margin-top:15px">
<b><font size=4>
<span style="color:black">
Microsoft Fellowship Nomination, 2022<br/> 
Postgraduate Scholarship, 2020-2024<br/> 
National Scholarship, 2019<br/> 
National Scholarship, 2016<br/> 
</span>
</font></b>
</p>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5u9eqiy07hd&amp;m=7&amp;c=007eff&amp;cr1=00fff6&amp;f=arial&amp;l=33" async="async"></script>